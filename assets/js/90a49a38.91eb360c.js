"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1754],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>m});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),m=r,f=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return t?a.createElement(f,i(i({ref:n},p),{},{components:t})):a.createElement(f,i({ref:n},p))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},81:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(7462),r=(t(7294),t(3905));const o={},i="Installing Drivers and Frameworks",s={unversionedId:"scaling/remote-tasks/installing-drivers-and-frameworks",id:"scaling/remote-tasks/installing-drivers-and-frameworks",title:"Installing Drivers and Frameworks",description:"Paradoxically, often the hardest part of using an hardware accelerator is to get all the",source:"@site/docs/scaling/remote-tasks/installing-drivers-and-frameworks.md",sourceDirName:"scaling/remote-tasks",slug:"/scaling/remote-tasks/installing-drivers-and-frameworks",permalink:"/scaling/remote-tasks/installing-drivers-and-frameworks",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/remote-tasks/installing-drivers-and-frameworks.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Using GPUs and Other Accelerators",permalink:"/scaling/remote-tasks/gpu-compute"},next:{title:"Distributed Computing",permalink:"/scaling/remote-tasks/distributed-computing"}},l={},c=[{value:"Using a GPU-ready Docker image",id:"using-a-gpu-ready-docker-image",level:2},{value:"Installing libraries with <code>@conda</code> and <code>@pypi</code>",id:"installing-libraries-with-conda-and-pypi",level:2}],p={toc:c};function d(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"installing-drivers-and-frameworks"},"Installing Drivers and Frameworks"),(0,r.kt)("p",null,"Paradoxically, often the hardest part of using an hardware accelerator is to get all the\nnecessary software installed, such as CUDA drivers and platform-specific ML/AI frameworks."),(0,r.kt)("p",null,"Metaflow allows you to ",(0,r.kt)("a",{parentName:"p",href:"/scaling/dependencies"},"specify software dependencies as a part of the flow"),".\nYou can either use a Docker image with necessary dependenices included, or layer them on top of\na generic image on the fly using ",(0,r.kt)("inlineCode",{parentName:"p"},"@conda")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"@pypi")," decorators. We cover both the approaches below."),(0,r.kt)("h2",{id:"using-a-gpu-ready-docker-image"},"Using a GPU-ready Docker image"),(0,r.kt)("p",null,"You can use the ",(0,r.kt)("inlineCode",{parentName:"p"},"image")," argument in ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes")," decorators to choose a suitable\nimage on the fly, like ",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/pytorch/pytorch"},"an official ",(0,r.kt)("inlineCode",{parentName:"a"},"pytorch")," image"),"\nwe use below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, step, kubernetes\n\nclass GPUImageFlow(FlowSpec):\n\n    @kubernetes(\n        gpu=1, \n        image='pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime'\n    )\n    @step\n    def start(self):\n        import torch # pylint: disable=import-error\n        if torch.cuda.is_available():\n            print('Cuda found \ud83d\ude4c')\n            for d in range(torch.cuda.device_count()):\n                print(f\"GPU device {d}:\", torch.cuda.get_device_name(d))\n        else:\n            print('No CUDA \ud83d\ude2d')\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == '__main__':\n    GPUImageFlow()\n")),(0,r.kt)("p",null,"If you want to avoid spec an image in the code, you can configure a default image in your ",(0,r.kt)("a",{parentName:"p",href:"https://outerbounds.com/engineering/operations/configure-metaflow/"},"Metaflow\nconfiguration file")," through the\n",(0,r.kt)("inlineCode",{parentName:"p"},"METAFLOW_KUBERNETES_CONTAINER_IMAGE")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"METAFLOW_BATCH_CONTAINER_IMAGE")," settings."),(0,r.kt)("p",null,"Many GPU-ready images are available online, e.g. at:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://catalog.ngc.nvidia.com/containers"},"Nvidia's NVCR catalogs"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/pytorch/pytorch"},"PyTorch DockerHub Registry"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://hub.docker.com/r/tensorflow/tensorflow"},"TensorFlow DockerHub Registry"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/aws/deep-learning-containers/blob/master/available_images.md"},"AWS' registry of Docker images for deep\nlearning"))),(0,r.kt)("p",null,"You can also ",(0,r.kt)("a",{parentName:"p",href:"https://outerbounds.com/docs/build-custom-image/"},"build a Docker image of your own"),",\nusing a GPU-ready image as a base image."),(0,r.kt)("h2",{id:"installing-libraries-with-conda-and-pypi"},"Installing libraries with ",(0,r.kt)("inlineCode",{parentName:"h2"},"@conda")," and ",(0,r.kt)("inlineCode",{parentName:"h2"},"@pypi")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"/scaling/dependencies/libraries"},"The ",(0,r.kt)("inlineCode",{parentName:"a"},"@conda")," and ",(0,r.kt)("inlineCode",{parentName:"a"},"@pypi")," decorators")," allow you to install\npackages on the fly on top of a default image. This makes it easy to test different libraries\nquickly without having to build custom images."),(0,r.kt)("p",null,"The CUDA drivers are hosted at ",(0,r.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation"},"NVIDIA's official Conda\nchannel"),".\nRun this command once to include the channel in your environment: "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"conda config --add channels nvidia\n")),(0,r.kt)("p",null,"After this, you can install PyTorch and other CUDA-enabled libraries with ",(0,r.kt)("inlineCode",{parentName:"p"},"@conda")," and\n",(0,r.kt)("inlineCode",{parentName:"p"},"@conda_base")," as usual. Try this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, resources, conda_base\n\n@conda_base(\n    libraries={\n        "pytorch::pytorch": "2.0.1",\n        "pytorch::pytorch-cuda": "11.8"\n    },\n    python="3.9"\n)\nclass GPUCondaFlow(FlowSpec):\n\n    @resources(gpu=1)\n    @step\n    def start(self):\n        import torch # pylint: disable=import-error\n        if torch.cuda.is_available():\n            print(\'Cuda found \ud83d\ude4c\')\n            for d in range(torch.cuda.device_count()):\n                print(f"GPU device {d}:", torch.cuda.get_device_name(d))\n        else:\n            print(\'No CUDA \ud83d\ude2d\')\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == \'__main__\':\n    GPUCondaFlow()\n')),(0,r.kt)("p",null,"Run the flow as"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"python gpuconda.py run --with batch\n")),(0,r.kt)("p",null,"or ",(0,r.kt)("inlineCode",{parentName:"p"},"--with kubernetes"),". When you run the flow for the first time, it will create an\nexecution environment and cache it, which will take a few minutes. Subsequent runs will\nstart faster."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"If you run workflows from a machine with a different operating system\nthan where remote tasks run, for example launching Metaflow runs that have remote\n",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes")," tasks from a Mac, some available dependencies and versions may not be\nthe same for each operating system. In this case, you can go to\nthe ",(0,r.kt)("a",{parentName:"p",href:"https://conda-forge.org/feedstock-outputs/"},"conda-forge website")," and find\nwhich package versions are available across each platform.")))}d.isMDXComponent=!0}}]);