"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[2872],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var i=a.createContext({}),u=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(i.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=u(n),d=o,h=m["".concat(i,".").concat(d)]||m[d]||c[d]||r;return n?a.createElement(h,s(s({ref:t},p),{},{components:n})):a.createElement(h,s({ref:t},p))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=m;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l.mdxType="string"==typeof e?e:o,s[1]=l;for(var u=2;u<r;u++)s[u]=n[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5162:(e,t,n)=>{n.d(t,{Z:()=>s});var a=n(7294),o=n(6010);const r="tabItem_Ymn6";function s(e){let{children:t,hidden:n,className:s}=e;return a.createElement("div",{role:"tabpanel",className:(0,o.Z)(r,s),hidden:n},t)}},5488:(e,t,n)=>{n.d(t,{Z:()=>d});var a=n(7462),o=n(7294),r=n(6010),s=n(2389),l=n(7392),i=n(7094),u=n(2466);const p="tabList__CuJ",c="tabItem_LNqP";function m(e){var t;const{lazy:n,block:s,defaultValue:m,values:d,groupId:h,className:f}=e,k=o.Children.map(e.children,(e=>{if((0,o.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),g=d??k.map((e=>{let{props:{value:t,label:n,attributes:a}}=e;return{value:t,label:n,attributes:a}})),b=(0,l.l)(g,((e,t)=>e.value===t.value));if(b.length>0)throw new Error(`Docusaurus error: Duplicate values "${b.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const y=null===m?m:m??(null==(t=k.find((e=>e.props.default)))?void 0:t.props.value)??k[0].props.value;if(null!==y&&!g.some((e=>e.value===y)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${y}" but none of its children has the corresponding value. Available values are: ${g.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:v,setTabGroupChoices:w}=(0,i.U)(),[N,x]=(0,o.useState)(y),C=[],{blockElementScrollPositionUntilNextRender:T}=(0,u.o5)();if(null!=h){const e=v[h];null!=e&&e!==N&&g.some((t=>t.value===e))&&x(e)}const _=e=>{const t=e.currentTarget,n=C.indexOf(t),a=g[n].value;a!==N&&(T(t),x(a),null!=h&&w(h,String(a)))},S=e=>{var t;let n=null;switch(e.key){case"ArrowRight":{const t=C.indexOf(e.currentTarget)+1;n=C[t]??C[0];break}case"ArrowLeft":{const t=C.indexOf(e.currentTarget)-1;n=C[t]??C[C.length-1];break}}null==(t=n)||t.focus()};return o.createElement("div",{className:(0,r.Z)("tabs-container",p)},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":s},f)},g.map((e=>{let{value:t,label:n,attributes:s}=e;return o.createElement("li",(0,a.Z)({role:"tab",tabIndex:N===t?0:-1,"aria-selected":N===t,key:t,ref:e=>C.push(e),onKeyDown:S,onFocus:_,onClick:_},s,{className:(0,r.Z)("tabs__item",c,null==s?void 0:s.className,{"tabs__item--active":N===t})}),n??t)}))),n?(0,o.cloneElement)(k.filter((e=>e.props.value===N))[0],{className:"margin-top--md"}):o.createElement("div",{className:"margin-top--md"},k.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==N})))))}function d(e){const t=(0,s.Z)();return o.createElement(m,(0,a.Z)({key:String(t)},e))}},720:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>u,toc:()=>c});var a=n(7462),o=(n(7294),n(3905)),r=n(5488),s=n(5162);const l={},i="Requesting Compute Resources",u={unversionedId:"scaling/remote-tasks/requesting-resources",id:"scaling/remote-tasks/requesting-resources",title:"Requesting Compute Resources",description:"You can run any Metaflow flow in the cloud simply by adding an option on the command line:",source:"@site/docs/scaling/remote-tasks/requesting-resources.md",sourceDirName:"scaling/remote-tasks",slug:"/scaling/remote-tasks/requesting-resources",permalink:"/scaling/remote-tasks/requesting-resources",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/remote-tasks/requesting-resources.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Computing at Scale",permalink:"/scaling/remote-tasks/introduction"},next:{title:"Using Multiple CPU Cores",permalink:"/scaling/remote-tasks/multicore"}},p={},c=[{value:"Example",id:"example",level:2},{value:"Moving between compute environments",id:"moving-between-compute-environments",level:2},{value:"Mixing local and remote compute",id:"mixing-local-and-remote-compute",level:3},{value:"Mixing cloud environments",id:"mixing-cloud-environments",level:3}],m={toc:c};function d(e){let{components:t,...l}=e;return(0,o.kt)("wrapper",(0,a.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"requesting-compute-resources"},"Requesting Compute Resources"),(0,o.kt)("p",null,"You can run any Metaflow flow in the cloud simply by adding an option on the command line:"),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"k8s",label:"Kubernetes",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-batch"},"$ python hello.py run --with kubernetes\n"))),(0,o.kt)(s.Z,{value:"batch",label:"AWS Batch",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-k8s"},"$ python hello.py run --with batch\n")))),(0,o.kt)("p",null,"When you add ",(0,o.kt)("inlineCode",{parentName:"p"},"--with kubernetes")," (for Kubernetes) or ",(0,o.kt)("inlineCode",{parentName:"p"},"--with batch")," (for AWS Batch) on the\ncommand line (",(0,o.kt)("a",{parentName:"p",href:"/getting-started/infrastructure"},"depending on your deployment"),"), Metaflow\nruns the flow on the chosen compute backend."),(0,o.kt)("p",null,"Every step gets allocated a modest amount of resources by default - around 1 CPU core and 4GB of\nRAM. If your step needs more CPU cores, memory, disk, or ",(0,o.kt)("a",{parentName:"p",href:"gpu-compute"},"more GPUs (or other hardware\naccelerators)"),", annotate your resource requirements with the\n",(0,o.kt)("a",{parentName:"p",href:"/api/step-decorators/resources"},(0,o.kt)("inlineCode",{parentName:"a"},"@resources"))," decorator."),(0,o.kt)("p",null,"Another benefit of ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," is that it allows you to move smoothly between local\ndevelopment and the cloud. The decorator doesn't have an effect for local runs, but when\ncombined with ",(0,o.kt)("inlineCode",{parentName:"p"},"--with kubernetes")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"--with batch"),", you can use the flow to handle bigger\nmodels or more data without changing anything in the code. Note that\n",(0,o.kt)("a",{parentName:"p",href:"/production/introduction"},"production deployments")," always run in the cloud, respecting\n",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," requirements."),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"@kubernetes")," can target any Kubernetes cluster, including on-premise clusters.\nFor brevity, we use the term ",(0,o.kt)("em",{parentName:"p"},"the cloud")," to refer to all compute backends.")),(0,o.kt)("h2",{id:"example"},"Example"),(0,o.kt)("p",null,"Consider the following example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, resources\n\nclass BigSum(FlowSpec):\n\n    @resources(memory=60000, cpu=1)\n    @step\n    def start(self):\n        import numpy\n        import time\n        big_matrix = numpy.random.ranf((80000, 80000))\n        t = time.time()\n        self.sum = numpy.sum(big_matrix)\n        self.took = time.time() - t\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print("The sum is %f." % self.sum)\n        print("Computing it took %dms." % (self.took * 1000))\n\nif __name__ == \'__main__\':\n    BigSum()\n')),(0,o.kt)("p",null,"This example creates a huge 80000x80000 random matrix, ",(0,o.kt)("inlineCode",{parentName:"p"},"big_matrix"),". The matrix requires\nabout 80000^2 ","*"," 8 bytes = 48GB of memory. "),(0,o.kt)("p",null,"If you attempt to run this on your local machine, it is likely that the following will\nhappen:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ python BigSum.py run\n\n2019-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "BugSum.py", line 11, in start\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] big_matrix = numpy.random.ranf((80000, 80000))\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "mtrand.pyx", line 856, in mtrand.RandomState.random_sample\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "mtrand.pyx", line 167, in mtrand.cont0_array\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] MemoryError\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)]\n2018-11-29 02:43:39.844 [5/start/21975 (pid 83812)] Task failed.\n2018-11-29 02:43:39.844 Workflow failed.\n    Step failure:\n    Step start (task-id 21975) failed.\n')),(0,o.kt)("p",null,"This fails quickly due to a ",(0,o.kt)("inlineCode",{parentName:"p"},"MemoryError")," on most laptops as we are unable to allocate\n48GB of memory. "),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," decorator suggests resource requirements for a step. The ",(0,o.kt)("inlineCode",{parentName:"p"},"memory"),"\nargument specifies the amount of RAM in megabytes and ",(0,o.kt)("inlineCode",{parentName:"p"},"cpu")," the number of CPU cores\nrequested. It does not produce the resources magically, which is why the run above\nfailed. The ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," decorator takes effect only when combined with another\ndecorator that describes what compute platform, like Kubernetes or AWS Batch, to use."),(0,o.kt)("p",null,"Let's use the ",(0,o.kt)("inlineCode",{parentName:"p"},"--with")," option to attach a desired decorator to all steps on the command\nline. Choose one of the commands in the tabs below corresponding to whichever you use-\nKubernetes or AWS Batch. This assumes that you have ",(0,o.kt)("a",{parentName:"p",href:"/getting-started/infrastructure"},"configured one of these systems\nwork with Metaflow"),"."),(0,o.kt)(r.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"k8s",label:"Kubernetes",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-batch"},"$ python BigSum.py run --with kubernetes\n"))),(0,o.kt)(s.Z,{value:"batch",label:"AWS Batch",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-k8s"},"$ python BigSum.py run --with batch\n")))),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"--with batch")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"--with kubernetes")," option instructs Metaflow to run all tasks as\nseparate jobs on the chosen compute platform, instead of using a local process for each\ntask. It has the same effect as adding the decorator above all steps in the source code."),(0,o.kt)("p",null,"This time the run should succeed thanks to the large enough instance, assuming a large\nenough instance is available in your compute environment. In this case the ",(0,o.kt)("inlineCode",{parentName:"p"},"resources"),"\ndecorator is used as a prescription for the size of the instance that the job should run\non. Make sure that this resource requirement can be met. If a large enough instance is\nnot available, the task won't start executing."),(0,o.kt)("p",null,"You should see an output like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"The sum is 3200003911.795288.\nComputing it took 4497ms.\n")),(0,o.kt)("p",null,"In addition to ",(0,o.kt)("inlineCode",{parentName:"p"},"cpu")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"memory")," you can specify ",(0,o.kt)("inlineCode",{parentName:"p"},"gpu=N")," to request N GPUs for the\ninstance."),(0,o.kt)("h2",{id:"moving-between-compute-environments"},"Moving between compute environments"),(0,o.kt)("p",null,"Metaflow makes it easy to mix and match compute environments. You can move from\nlocal prototyping to cloud execution easily, but also ",(0,o.kt)("a",{parentName:"p",href:"https://outerbounds.com/blog/metaflow-on-all-major-clouds/"},"mix different cloud\ncompute backends")," fluidly."),(0,o.kt)("h3",{id:"mixing-local-and-remote-compute"},"Mixing local and remote compute"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"resources")," decorator is an annotation that signals how much resources are required\nby a step. By itself, it does not force the step to be executed on any particular\nplatform. This is convenient as you can make the choice later, executing the same flow\non different environments without changes."),(0,o.kt)("p",null,"For instance, we can take the above example and replace ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"@batch"),"\n(or ",(0,o.kt)("inlineCode",{parentName:"p"},"@kubernetes"),"):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, resources\n\nclass BigSum(FlowSpec):\n\n    @batch(memory=60000, cpu=1)\n    @step\n    def start(self):\n        import numpy\n        import time\n        big_matrix = numpy.random.ranf((80000, 80000))\n        t = time.time()\n        self.sum = numpy.sum(big_matrix)\n        self.took = time.time() - t\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print("The sum is %f." % self.sum)\n        print("Computing it took %dms." % (self.took * 1000))\n\nif __name__ == \'__main__\':\n    BigSum()\n')),(0,o.kt)("p",null,"In contrast to ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources"),", the ",(0,o.kt)("inlineCode",{parentName:"p"},"@batch")," decorator (and ",(0,o.kt)("inlineCode",{parentName:"p"},"@kubernetes"),") forces\nthe step to be executed remotely. Run the flow without ",(0,o.kt)("inlineCode",{parentName:"p"},"--with")," option:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ python BigSum.py run\n")),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(9751).Z,width:"573",height:"278"})),(0,o.kt)("p",null,"You will see that the ",(0,o.kt)("inlineCode",{parentName:"p"},"start")," step gets executed on AWS Batch but the ",(0,o.kt)("inlineCode",{parentName:"p"},"end")," step,\nwhich does not need special resources, is executed locally."),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"Mixing local and remote steps can speed up development cycles,\nas you can execute some  steps locally with minimal overhead, even\naccessing local files, while executing only demanding steps in the cloud.\nMetaflow takes care of moving data between the environments automatically.")),(0,o.kt)("h3",{id:"mixing-cloud-environments"},"Mixing cloud environments"),(0,o.kt)("p",null,"You can mix and match ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"@batch"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"@kubernetes")," freely, which makes\nit possible to create advanced workflows that leverage multiple compute environments\nfrom workstations and on-prem data centers to the public cloud. Just ",(0,o.kt)("a",{parentName:"p",href:"/getting-started/infrastructure"},"set up\nyour Metaflow stack")," to support all the environments you want to use."),(0,o.kt)("p",null,"As a hypothetical example, consider this flow that mixes local compute, on-prem\ncompute, and various forms of cloud compute:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import random\nfrom metaflow import FlowSpec, step, resources, kubernetes, card\n\nclass HybridCloudFlow(FlowSpec):\n\n    @step\n    def start(self):\n        self.countries = ['US', 'BR', 'IT']\n        self.shards = {country: open(f'{country}.data').read()\n                       for country in self.countries}\n        self.next(self.prepare_data, foreach='countries')\n\n    @kubernetes(memory=16000)\n    @step\n    def prepare_data(self):\n        print('processing a shard of data', self.shards[self.input])\n        self.next(self.train)\n\n    @batch(gpu=2, queue='gpu-queue')\n    @step\n    def train(self):\n        print('training model...')\n        self.score = random.randint(0, 10)\n        self.country = self.input\n        self.next(self.join)\n\n    @batch(memory=16000, queue='cpu-queue')    \n    @step\n    def join(self, inputs):\n        self.best = max(inputs, key=lambda x: x.score).country\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(self.best, 'produced best results')\n\nif __name__ == '__main__':\n    HybridCloudFlow()\n")),(0,o.kt)("p",null,"Here's an illustration of the flow:"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(3446).Z,width:"960",height:"383"})),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"start")," executes locally, loading local files for processing. Data is separated\nin three shards, one for each country.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"prepare_data")," leverages an on-prem Kubernetes cluster to preprocess data, say, due\nto data privacy reasons.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"train")," uses ",(0,o.kt)("a",{parentName:"p",href:"gpu-compute"},"cloud GPUs")," to train a model per country in parallel.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"join")," loads the models in a high-memory cloud instance, evaluates them, and\nchooses the best performing country.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"end")," fetches the results back to the local laptop."))),(0,o.kt)("p",null,"Metaflow takes care of ",(0,o.kt)("a",{parentName:"p",href:"/scaling/dependencies"},"packaging code")," automatically, it\nremoves the need to move data manually, and it tracks all metadata consistently, across\nthe environments."))}d.isMDXComponent=!0},9751:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/cloud-and-local-29de32b2b8ad238debca508a3d9b3da3.png"},3446:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/hybrid_cloud-0d01755c60ec8b5565186904fd28f17a.png"}}]);