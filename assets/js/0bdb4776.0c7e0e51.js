"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1099],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>d});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(a),d=r,h=c["".concat(s,".").concat(d)]||c[d]||m[d]||o;return a?n.createElement(h,l(l({ref:t},u),{},{components:a})):n.createElement(h,l({ref:t},u))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=c;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var p=2;p<o;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},5162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(7294),r=a(6010);const o="tabItem_Ymn6";function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o,l),hidden:a},t)}},5488:(e,t,a)=>{a.d(t,{Z:()=>d});var n=a(7462),r=a(7294),o=a(6010),l=a(2389),i=a(7392),s=a(7094),p=a(2466);const u="tabList__CuJ",m="tabItem_LNqP";function c(e){var t;const{lazy:a,block:l,defaultValue:c,values:d,groupId:h,className:f}=e,k=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),g=d??k.map((e=>{let{props:{value:t,label:a,attributes:n}}=e;return{value:t,label:a,attributes:n}})),y=(0,i.l)(g,((e,t)=>e.value===t.value));if(y.length>0)throw new Error(`Docusaurus error: Duplicate values "${y.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const b=null===c?c:c??(null==(t=k.find((e=>e.props.default)))?void 0:t.props.value)??k[0].props.value;if(null!==b&&!g.some((e=>e.value===b)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${b}" but none of its children has the corresponding value. Available values are: ${g.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:w,setTabGroupChoices:v}=(0,s.U)(),[N,C]=(0,r.useState)(b),x=[],{blockElementScrollPositionUntilNextRender:T}=(0,p.o5)();if(null!=h){const e=w[h];null!=e&&e!==N&&g.some((t=>t.value===e))&&C(e)}const _=e=>{const t=e.currentTarget,a=x.indexOf(t),n=g[a].value;n!==N&&(T(t),C(n),null!=h&&v(h,String(n)))},S=e=>{var t;let a=null;switch(e.key){case"ArrowRight":{const t=x.indexOf(e.currentTarget)+1;a=x[t]??x[0];break}case"ArrowLeft":{const t=x.indexOf(e.currentTarget)-1;a=x[t]??x[x.length-1];break}}null==(t=a)||t.focus()};return r.createElement("div",{className:(0,o.Z)("tabs-container",u)},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":l},f)},g.map((e=>{let{value:t,label:a,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:N===t?0:-1,"aria-selected":N===t,key:t,ref:e=>x.push(e),onKeyDown:S,onFocus:_,onClick:_},l,{className:(0,o.Z)("tabs__item",m,null==l?void 0:l.className,{"tabs__item--active":N===t})}),a??t)}))),a?(0,r.cloneElement)(k.filter((e=>e.props.value===N))[0],{className:"margin-top--md"}):r.createElement("div",{className:"margin-top--md"},k.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==N})))))}function d(e){const t=(0,l.Z)();return r.createElement(c,(0,n.Z)({key:String(t)},e))}},6416:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>p,toc:()=>m});var n=a(7462),r=(a(7294),a(3905)),o=a(5488),l=a(5162);const i={},s="Executing Tasks Remotely",p={unversionedId:"scaling/remote-tasks/introduction",id:"scaling/remote-tasks/introduction",title:"Executing Tasks Remotely",description:"There are two ways to handle larger amounts of data and compute:",source:"@site/docs/scaling/remote-tasks/introduction.md",sourceDirName:"scaling/remote-tasks",slug:"/scaling/remote-tasks/introduction",permalink:"/scaling/remote-tasks/introduction",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/remote-tasks/introduction.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Scalable Compute and Data",permalink:"/scaling/introduction"},next:{title:"Using Kubernetes",permalink:"/scaling/remote-tasks/kubernetes"}},u={},m=[{value:"Requesting resources with <code>resources</code> decorator",id:"requesting-resources-with-resources-decorator",level:2},{value:"Running only specific steps remotely",id:"running-only-specific-steps-remotely",level:3},{value:"Parallelization over multiple cores",id:"parallelization-over-multiple-cores",level:3},{value:"Parallel map",id:"parallel-map",level:4},{value:"<strong>Safeguard flags</strong>",id:"safeguard-flags",level:2},{value:"Big Data",id:"big-data",level:2}],c={toc:m};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"executing-tasks-remotely"},"Executing Tasks Remotely"),(0,r.kt)("p",null,"There are two ways to handle larger amounts of data and compute:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Scale up")," by running your code on a larger machine with more memory, CPU cores, and\nGPUs, or"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("em",{parentName:"li"},"Scale out")," by using more machines in parallel.")),(0,r.kt)("p",null,"As described below, Metaflow supports the former through the ",(0,r.kt)("inlineCode",{parentName:"p"},"@resources")," decorator and\nthe latter through ",(0,r.kt)("a",{parentName:"p",href:"/metaflow/basics#foreach"},"foreach")," when flows are run on Kubernetes\nor AWS Batch."),(0,r.kt)("p",null,"Everything described on this page applies to all compute platforms supported by\nMetaflow. The data scientist can write their flows using foreaches and the ",(0,r.kt)("inlineCode",{parentName:"p"},"@resource"),"\ndecorator knowing that the code will execute on any supported platforms. For additional\ntips and tricks related to specific systems, see ",(0,r.kt)("a",{parentName:"p",href:"aws-batch"},"Using AWS Batch")," and ",(0,r.kt)("a",{parentName:"p",href:"kubernetes"},"Using\nKubernetes"),"."),(0,r.kt)("h2",{id:"requesting-resources-with-resources-decorator"},"Requesting resources with ",(0,r.kt)("inlineCode",{parentName:"h2"},"resources")," decorator"),(0,r.kt)("p",null,"Consider the following example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, resources\n\nclass BigSum(FlowSpec):\n\n    @resources(memory=60000, cpu=1)\n    @step\n    def start(self):\n        import numpy\n        import time\n        big_matrix = numpy.random.ranf((80000, 80000))\n        t = time.time()\n        self.sum = numpy.sum(big_matrix)\n        self.took = time.time() - t\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print("The sum is %f." % self.sum)\n        print("Computing it took %dms." % (self.took * 1000))\n\nif __name__ == \'__main__\':\n    BigSum()\n')),(0,r.kt)("p",null,"This example creates a huge 80000x80000 random matrix, ",(0,r.kt)("inlineCode",{parentName:"p"},"big_matrix"),". The matrix requires\nabout 80000^2 ","*"," 8 bytes = 48GB of memory. "),(0,r.kt)("p",null,"If you attempt to run this on your local machine, it is likely that the following will\nhappen:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ python BigSum.py run\n\n2019-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "BugSum.py", line 11, in start\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] big_matrix = numpy.random.ranf((80000, 80000))\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "mtrand.pyx", line 856, in mtrand.RandomState.random_sample\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File "mtrand.pyx", line 167, in mtrand.cont0_array\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] MemoryError\n2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)]\n2018-11-29 02:43:39.844 [5/start/21975 (pid 83812)] Task failed.\n2018-11-29 02:43:39.844 Workflow failed.\n    Step failure:\n    Step start (task-id 21975) failed.\n')),(0,r.kt)("p",null,"This fails quickly due to a ",(0,r.kt)("inlineCode",{parentName:"p"},"MemoryError")," on most laptops as we are unable to allocate\n48GB of memory. "),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"@resources")," decorator suggests resource requirements for a step. The ",(0,r.kt)("inlineCode",{parentName:"p"},"memory"),"\nargument specifies the amount of RAM in megabytes and ",(0,r.kt)("inlineCode",{parentName:"p"},"cpu")," the number of CPU cores\nrequested. It does not produce the resources magically, which is why the run above\nfailed. The ",(0,r.kt)("inlineCode",{parentName:"p"},"@resources")," decorator takes effect only when combined with another\ndecorator that describes what compute platform, like Kubernetes or AWS Batch, to use."),(0,r.kt)("p",null,"Let's use the ",(0,r.kt)("inlineCode",{parentName:"p"},"--with")," option to attach a desired decorator to all steps on the command\nline. Choose one of the commands in the tabs below corresponding to whichever you use-\nKubernetes or AWS Batch. This assumes that you have ",(0,r.kt)("a",{parentName:"p",href:"/getting-started/infrastructure"},"configured one of these systems\nwork with Metaflow"),"."),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"k8s",label:"Kubernetes",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-batch"},"$ python BigSum.py run --with kubernetes\n"))),(0,r.kt)(l.Z,{value:"batch",label:"AWS Batch",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-k8s"},"$ python BigSum.py run --with batch\n")))),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"--with batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"--with kubernetes")," option instructs Metaflow to run all tasks as\nseparate jobs on the chosen compute platform, instead of using a local process for each\ntask. It has the same effect as adding the decorator above all steps in the source code."),(0,r.kt)("p",null,"This time the run should succeed thanks to the large enough instance, assuming a large\nenough instance is available in your compute environment. In this case the ",(0,r.kt)("inlineCode",{parentName:"p"},"resources"),"\ndecorator is used as a prescription for the size of the instance that the job should run\non. Make sure that this resource requirement can be met. If a large enough instance is\nnot available, the task won't start executing."),(0,r.kt)("p",null,"You should see an output like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"The sum is 3200003911.795288.\nComputing it took 4497ms.\n")),(0,r.kt)("p",null,"In addition to ",(0,r.kt)("inlineCode",{parentName:"p"},"cpu")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"memory")," you can specify ",(0,r.kt)("inlineCode",{parentName:"p"},"gpu=N")," to request N GPUs for the\ninstance."),(0,r.kt)("h3",{id:"running-only-specific-steps-remotely"},"Running only specific steps remotely"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"resources")," decorator is an annotation that signals how much resources are required\nby a step. By itself, it does not force the step to be executed on any particular\nplatform. This is convenient as you can make the choice later, executing the same flow\non different environments without changes."),(0,r.kt)("p",null,"Sometimes it is useful to make sure that a step always executes on a certain compute\nplatform, maybe using a platform-specific configuration. You can achieve this by adding\neither ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes")," above steps that should be executed remotely. The\ndecorators accept the same keyword arguments as ",(0,r.kt)("inlineCode",{parentName:"p"},"@resources")," as well as\nplatform-specific arguments that you can find listed in ",(0,r.kt)("a",{parentName:"p",href:"/api/step-decorators"},"the API\nreference"),"."),(0,r.kt)("p",null,"For instance, in the example above, replace ",(0,r.kt)("inlineCode",{parentName:"p"},"@resources")," with ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes"),"\nand run it as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ python BigSum.py run\n")),(0,r.kt)("p",null,"You will see that the ",(0,r.kt)("inlineCode",{parentName:"p"},"start")," step gets executed on a remote instance but the ",(0,r.kt)("inlineCode",{parentName:"p"},"end"),"\nstep, which does not need special resources, is executed locally. You could even mix\ndecorators so that some steps execute on ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes"),", some on ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch"),", and some\nlocally."),(0,r.kt)("h3",{id:"parallelization-over-multiple-cores"},"Parallelization over multiple cores"),(0,r.kt)("p",null,"When running locally, tasks are executed as separate processes. The operating system\ntakes care of allocating them to separate CPU cores, so they will actually execute in\nparallel assuming that enough CPU cores are available. Hence, your flow can utilize\nmultiple cores without you having to do anything special besides defining branches in\nthe flow."),(0,r.kt)("p",null,"When running remotely on ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes"),", branches are mapped to separate jobs\nthat are executed in parallel, allowing you to ",(0,r.kt)("em",{parentName:"p"},"scale horizontally")," to any number of\nparallel tasks. In addition, you may take advantage of multiple CPU cores inside a task.\nThis may happen automatically if you use a modern ML library like PyTorch or Scikit\nLearn, or you may parallelize functions explicitly, as explained below."),(0,r.kt)("h4",{id:"parallel-map"},"Parallel map"),(0,r.kt)("p",null,"Metaflow provides a utility function called ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," that helps take advantage of\nmultiple CPU cores. This function is almost equivalent to ",(0,r.kt)("inlineCode",{parentName:"p"},"Pool().map")," in the Python's\nbuilt-in\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.map"},"multiprocessing"),"\nlibrary. The main differences are the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," supports lambdas and any other callables of Python."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," does not suffer from bugs present in ",(0,r.kt)("inlineCode",{parentName:"li"},"multiprocessing"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," can handle larger amounts of data.")),(0,r.kt)("p",null,"You may also use ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," to parallelize simple operations that might be too\ncumbersome to implement as separate steps."),(0,r.kt)("p",null,"Here is an extension of our previous example that implements a multicore ",(0,r.kt)("inlineCode",{parentName:"p"},"sum()")," by\npartitioning the matrix by row:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, batch, parallel_map\n\nclass BigSum(FlowSpec):\n\n    @resources(memory=60000, cpu=8)\n    @step\n    def start(self):\n        import numpy\n        import time\n        big_matrix = numpy.random.ranf((80000, 80000))\n        t = time.time()\n        parts = parallel_map(lambda i: big_matrix[i:i+10000].sum(),\n                             range(0, 80000, 10000))\n        self.sum = sum(parts)\n        self.took = time.time() - t\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print("The sum is %f." % self.sum)\n        print("Computing it took %dms." % (self.took * 1000))\n\nif __name__ == \'__main__\':\n    BigSum()\n')),(0,r.kt)("p",null,"Note that we use ",(0,r.kt)("inlineCode",{parentName:"p"},"cpu=8")," to request enough CPU cores, so our ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," can benefit\nfrom optimal parallelism. Disappointingly, in this case the parallel ",(0,r.kt)("inlineCode",{parentName:"p"},"sum")," is not faster\nthan the original simple implementation due to the overhead of launching separate\nprocesses in ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map"),". A less trivial operation might see a much larger\nperformance boost."),(0,r.kt)("h2",{id:"safeguard-flags"},(0,r.kt)("strong",{parentName:"h2"},"Safeguard flags")),(0,r.kt)("p",null,"It is almost too easy to execute tasks remotely using Metaflow. Consider a foreach loop\ndefined as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"self.params = range(1000)\nself.next(self.fanned_out, foreach='params')\n")),(0,r.kt)("p",null,"When run with ",(0,r.kt)("inlineCode",{parentName:"p"},"--with batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"--with kubernetes"),", this code would launch up to 1000\nparallel instances which may turn out to be quite expensive."),(0,r.kt)("p",null,"To safeguard against inadvertent launching of many parallel jobs, the ",(0,r.kt)("inlineCode",{parentName:"p"},"run")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"resume"),"\ncommands have a flag ",(0,r.kt)("inlineCode",{parentName:"p"},"--max-num-splits")," which fails the task if it attempts to launch\nmore than 100 splits by default. Use the flag to increase the limit if you actually need\nmore tasks."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ python myflow.py run --max-num-splits 200\n")),(0,r.kt)("p",null,"Another flag, ",(0,r.kt)("inlineCode",{parentName:"p"},"--max-workers"),", limits the number of tasks run in parallel. Even if a\nforeach launched 100 splits, ",(0,r.kt)("inlineCode",{parentName:"p"},"--max-workers")," would make only 16 ","(","by default",")"," of them\nrun in parallel at any point in time. If you want more parallelism, increase the value\nof ",(0,r.kt)("inlineCode",{parentName:"p"},"--max-workers"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ python myflow.py run --max-workers 32\n")),(0,r.kt)("h2",{id:"big-data"},"Big Data"),(0,r.kt)("p",null,"Thus far, we have focused on CPU and memory-bound steps. Loading and processing big data\nis often an IO-bound operation which requires a different approach. Read ",(0,r.kt)("a",{parentName:"p",href:"/scaling/data"},"Loading and\nStoring Data")," for more details about how to build efficient data\npipelines in Metaflow."))}d.isMDXComponent=!0}}]);